template_info:
  create: 6 # number of experiments to create
  include_keys: ['model', 'optimizer', 'scheduler', 'audio_chunking', 'wandb', 'checkpointing', 'data', 'training', 'sequence_scheduler', 'spec_augment'] # keys to include in experiments config
  template_keys: [
    'checkpointing.dir', 
    'wandb.name', 
    'model.meta_layer_grad_norm'
  ] # keys to use to create experiments

model:
  feat_in: 80
  n_layers: 6
  d_model: 512
  n_heads: 6
  head_dim: 64
  dropout_ff: 0.0
  dropout_attn: 0.0
  dropout_conv: 0.0
  subsampling_factor: 8
  subsampling: dw_striding
  subsampling_act: silu
  subsampling_conv_channels: 256
  subsampling_norm_out: false
  conv_kernel_size: 9
  qk_rms_norm: false
  self_conditioning: true
  gated_sc: false
  decoder_norm: true
  use_rotary: true
  encoder_mode: conformer 
  default_norm: layer_norm
  sandwich_norm: false
  bias_in_ff: false
  checkpoint_subsampling: false
  checkpoint_every_n_layers: 0
  flash_attn: true
  meta_layer_norm_out_type: 'layer_norm'
  meta_layer_gradclip: True
  meta_layer_grad_norm: [
    0.8,
    1.0,
    2.5,
    5.0,
    15.0,
    20.0
  ]

optimizer:
  name: 'madgrad'
  args:
    lr: 2e-3

scheduler:
  warmup_steps: 9000

audio_chunking: # 360000 (max = 1 hour)
  size: 1024
  overlap: 0 # not using currently...

sequence_scheduler:
  increase_every: 5000
  stop_after: 90000
  start_after: 0
  max_sequence_length: 8192
  increase_by_multiplier: 2
  batch_size_multiplier: 0.5
  interpolate_rotary: false

wandb:
  use: true
  project_name: "spotify_long_context"
  name: [ 
    'meta_l_gradnorm_p8', #1
    'meta_l_gradnorm_1', #2
    'meta_l_gradnorm_2p5', #3
    'meta_l_gradnorm_5', #4
    'meta_l_gradnorm_15', #5
    'meta_l_gradnorm_20', #6
  ]
  id: "" # leave empty if not resuming a previous run

checkpointing:
  dir: [
    '/mnt/parscratch/users/acp21rjf/spotify/checkpoints/meta_l_gradnorm_p8', #1
    '/mnt/parscratch/users/acp21rjf/spotify/checkpoints/meta_l_gradnorm_1', #2
    '/mnt/parscratch/users/acp21rjf/spotify/checkpoints/meta_l_gradnorm_2p5', #3
    '/mnt/parscratch/users/acp21rjf/spotify/checkpoints/meta_l_gradnorm_5', #4
    '/mnt/parscratch/users/acp21rjf/spotify/checkpoints/meta_l_gradnorm_15', #5
    '/mnt/parscratch/users/acp21rjf/spotify/checkpoints/meta_l_gradnorm_20', #6

  ]
  save_every_n_steps: 2000

# full data: #'/mnt/parscratch/users/acp21rjf/spotify/audio_txt_pairs.json'
# 25% of corpus: '/users/acp21rjf/long-context-asr/reduced_pairs.json'
data:
  path: '/mnt/parscratch/users/acp21rjf/spotify/audio_txt_pairs.json'
  

spec_augment:
  n_time_masks: 1 # Number of time masks. If its value is zero, no time masking will be applied 
  time_mask_param: -1 # Maximum possible length of the time mask if it is set to -1, min_p is used
  n_freq_masks: 3 # Number of frequency masks. If its value is zero, no frequency masking will be applied
  freq_mask_param: 32 # Maximum possible length of the frequency mask
  iid_masks: true # Applies iid masks to each of the examples in the batch dimension.
  min_p: 0.01 # minimum proportion of time steps that can be masked. Must be within range [0.0, 1.0]. (Default: -1 which means use fixed number of masks)
  max_p: 0.5 # maximum proportion of time steps that can be masked. Must be within range [0.0, 1.0]. (Default: 1.0)
  zero_masking: true # If true, masked values will be set to zero, otherwise they will be set to the mean value of the input spectrogram



training:
  start_spec_augment_after_n_epochs: -1 # -1 to disable
  batch_size: 224
  backprop_every: 1
  backwards_every: 1
  max_seq_len: 0
  clip_value: 0.8
  intermediate_loss_weighting: 0.0
  

# size: 512 = batch size 704
# size: 1024 = batch size 352
# size: 2048 = batch size 176
# size: 4096 = batch size 88
# size: 8192 = batch size 44
# size: 16384 = batch size 22
# size: 65536 = batch size 5 
# size: 131072 = batch size 2
# size: 360000 (1 hour) =  batch size 1